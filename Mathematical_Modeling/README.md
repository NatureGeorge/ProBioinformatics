# Mathematical ModelingğŸŒ€ğŸ”¥âš¡

```txt
Created Date: Thursday, September 5th 2019, 10:57:07 am
Author: ZeFeng Zhu
```

## Progress

> ğŸ‘¶ ğŸ‘¦ ğŸ§’ ğŸ‘¨ ğŸ‘´

|   |Mathematical Modeling|Pattern Recognition|Machine Learning|
|---|---|---|---|
|Basic Knowledge|ğŸ‘¦|ğŸ‘¦|ğŸ‘¶|
|Introduction|ğŸ‘¦|ğŸ‘¦|ğŸ‘¶|
|Practice|ğŸ‘¶|ğŸ‘¶|ğŸ‘¶|
|Pro|ğŸ‘¶|ğŸ‘¶|ğŸ‘¶|

## Some Note

> Noted that following note may be error-prone

### Entropy (Measuring Information)

#### Definition of Shannon's Entropy

* ä¿¡æ¯ä¸­æœ‰
  * æœ‰æ„ä¹‰ä¿¡æ¯(éå†—ä½™)
  * æ— æ„ä¹‰ä¿¡æ¯
  * å†—ä½™ä¿¡æ¯
* Bitä½œä¸ºä¿¡æ¯é‡(Information)çš„è¡¡é‡æ–¹å¼
  * Bit = 0 or 1 (æ˜¯æˆ–ä¸æ˜¯, å¤©ç„¶ç­‰æ¦‚ç‡)
  * Bit = Uncertainty divided by 2
* å¯¹äºBitè®¡ç®—æ–¹æ³•çš„ä¸¤ç§ç›´è§‚ç†è§£æ–¹å¼
  * Nç§æƒ…å†µ(state) -> logè®¡ç®— -> å¾—åˆ°ä¿¡æ¯é‡ $\log_{2}^{N}$
    * å¯ä»¥è¿™æ ·ç†è§£ï¼ŒNç§æƒ…å†µç­‰å¯èƒ½(Uncertainty divided by N, N is uncertainty reduction factor)ï¼ŒæŠŠæ‰€æœ‰æƒ…å†µèµ‹äºˆäºŒè¿›åˆ¶ç¼–ç , ä¸ºäº†å®Œæ•´æè¿°æ¯ä¸€æƒ…å†µï¼Œéœ€è¦çš„äºŒè¿›åˆ¶ä½æ•°å³æ˜¯ä¼ é€’è¯¥æƒ…å†µæ‰€éœ€çš„ä¿¡æ¯é‡
    * åœ¨æ­¤ç§ç†è§£æ–¹å¼ä¸‹
      * Nåªèƒ½ä¸ºæ­£æ•´æ•°?
      * åªé€‚ç”¨äºå„ä¸ªæƒ…å†µ(state)<å¯èƒ½æ€§/æ¦‚ç‡>å‡ç­‰çš„æƒ…å†µ?
  * ç›´æ¥logåŒ–æ¦‚ç‡ (MORE GENERAL) $-log_2^{p}$
    * ä»”ç»†æ€è€ƒä¼šçŸ¥é“ï¼Œ$\text{num of yes/no questions}=\log_{2}(\text{æƒ…å†µæ•°ç›®})=\log_{2}(1/p)$
    * ä¸è¿‡å½“éç­‰æ¦‚ç‡(ä¸”åº•æ•°é2çš„æŒ‡æ•°å€)ä»¥åŠæ¦‚ç‡å€’æ•°éæ•´æ•°çš„æƒ…å†µä¸‹ï¼Œè¯¥å®šä¹‰ä»é€‚ç”¨çš„ç†ç”±è¿˜è¦æ€è€ƒæ¸…æ¥š
* ä»logè®¡ç®—æ¥çœ‹, å¯¹äºæ¦‚ç‡è¶Šå¤§çš„äº‹ä»¶, å…¶å¯¹åº”ä¿¡æ¯bitè¶Šä½; stateè¶Šå°‘å¯¹åº”ä¿¡æ¯bitä¹Ÿè¶Šä½ã€‚åä¹‹è¶Šå¤§ã€‚æ„å‘³ç€è¶Šç¡®å®šçš„äº‹æƒ…bitè¶Šä½(ä¿¡æ¯é‡è¶Šå°‘)ï¼Œåä¹‹è¶Šå¤§ã€‚
* Measure the average amount of information
  * ç›¸å½“äºæ•°å­¦æœŸæœ› (average number of yes/no questions we need to ask to get the correct answer, å‚è§video2, äºŒå‰æ ‘çš„åº”ç”¨)
  * $-\sum_{i}p_{i}log_{2}(p_{i})$
  * That's it, the formula calculates the entropy, which measures the uncertainty.
  * $H(\text{p}) = -\sum_{i}p_{i}log_{2}(p_{i})$, $\text{p}$ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    * it tells how unpredictable the probability distribution is.
    * ä¸”åœ¨è¿™é‡Œçš„ä¿¡æ¯éƒ½æ˜¯æœ‰æ„ä¹‰ä¿¡æ¯ï¼Œæ— æ„ä¹‰æˆ–å†—ä½™ä¿¡æ¯å¯ä»¥çœ‹ä½œæ˜¯ä¸ºä¼ é€’æœ‰æ„ä¹‰ä¿¡æ¯è€Œäº§ç”Ÿçš„æˆæœ¬ï¼Œä¼ é€’çš„ä¿¡æ¯çš„é›†åˆç§°ä¸ºæ¶ˆæ¯(message)

|Index|Weather|Possibility|Code|Information|Message|
|---|---|---|---|---|---|
|0  |ğŸŒ |0.125|000|3 bits|3 bits|
|1  |â›… |0.125|001|3 bits|3 bits|
|2  |â˜ |0.125|010|3 bits|3 bits|
|3  |ğŸŒ¨ |0.125|011|3 bits|3 bits|
|4  |ğŸŒ§ |0.125|100|3 bits|3 bits|
|5  |ğŸŒ© |0.125|101|3 bits|3 bits|
|6  |ğŸŒª |0.125|110|3 bits|3 bits|
|7  |ğŸŒ« |0.125|111|3 bits|3 bits|

è¡¨1

```viz
graph tree1 {
    edge [color="0.700 0.200 1.000"];
    node [style=filled, color="0.650 0.200 1.000"];
    overlap=false;
    ABCD--AB[label="1/2"];
    ABCD--CD[label="1/2"];
    AB--A[label="1/4"];
    AB--B[label="1/4"];
    CD--C[label="1/4"];
    CD--D[label="1/4"];
}
```

å›¾1 (ç­‰æ¦‚ç‡ï¼Œå±‚å±‚äºŒåˆ†)

```viz
graph tree2 {
    edge [color="0.700 0.200 1.000"];
    node [style=filled, color="0.650 0.200 1.000"];
    ABCD--A[label="1/2"];
    ABCD--BCD[label="1/2"];
    BCD--D[label="1/4"];
    BCD--BC[label="1/4"];
    BC--B[label="1/8"];
    BC--C[label="1/8"];
}
```

å›¾2 (éç­‰æ¦‚ç‡ï¼Œä½†å¯å±‚å±‚äºŒåˆ†)

```viz
graph tree3 {
    edge [color="0.700 0.200 1.000"];
    node [style=filled, color="0.650 0.200 1.000"];
    A[label="3/5"];
    B[label="1/5"];
    C[label="1/10"];
    D[label="1/10"];
}
```

å›¾3 (éç­‰æ¦‚ç‡ï¼Œä¸å¯äºŒåˆ†?)

* Cross-Entropy
  * å¯ä»¥ç†è§£ä¸ºä¼ é€’æ¶ˆæ¯(message)æ‰€éœ€çš„æœŸæœ›ä¿¡æ¯é‡?
  * è¡¨2ä¸­ $\text{Entropy} = 2.23 \,\text{bits},\, \text{Cross-Entropy} = 3\,\text{bits}$
  * è¡¨3ä¸­ $\text{Cross-Entropy} = 2.42\,\text{bits}$
  * $H(\text{p, q})=-\sum_{i}p_{i}\log_{2}(q_{i})$, $\text{p}$ä¸ºtrueæ¦‚ç‡åˆ†å¸ƒ, $\text{q}$ä¸ºpredictedæ¦‚ç‡åˆ†å¸ƒ
  * æåŠçš„äºŒè¡¨çš„Messageåˆ—å³ä¸ºpredicted distributionï¼Œç”±Codeåˆ—å†³å®š
  * predicted possibilityåŠ å’Œä¸ä¸€å®šä¸º1
  * å¦‚æœé¢„æµ‹è‰¯å¥½(ä¸¤distributionç›¸ç­‰)ï¼Œåˆ™Cross-Entropyä¸Entropyè®¡ç®—æ•°å€¼ç›¸ç­‰; å¦‚æœpredicted distributionä¸true distributionå­˜åœ¨å·®å¼‚ï¼Œåˆ™Cross-Entropyå°†ä¼šæ¯”Entropyå¤§(åœ¨messageè¿›è¡Œåˆç†ç¼–ç çš„æƒ…å†µä¸‹?)
    * $\text{Cross-Entropy}-\text{Entropy} \Rightarrow \text{Relative Entropy} \Rightarrow \text{Kullback-Leibler divergence}$
    * $\text{Cross-Entropy} = \text{Entropy} + \text{K-L divergence}$
  * K-L divergence:
    * $D_{KL}(\text{p}||\text{q})=H(\text{p, q})-H(\text{p})=-\sum_{i}p_{i}\log_{2}(q_{i})+\sum_{i}p_{i}\log_{2}(p_{i})=\sum_{i}p_{i}[log_{2}(p_{i})-log_2(q_{i})]=\sum_{i}p_{i}\log_{2}^{\cfrac{p_{i}}{q_{i}}}$
    * å¦ä¸€ç§å†™æ³•: $D_{KL}(P||Q)=E_{x\sim P}\left[\log \cfrac{P(x)}{Q(x)}\right]$
    * ç‰¹æ®Šæƒ…å½¢: $D(p||q)=p\log^{\cfrac{p}{q}}+(1-p)\log^{\cfrac{1-p}{1-q}}$
    * Saying: "KL divergence from q to p"
    * Properties
      * $D_{KL}(P||Q)\ge 0, \forall P, Q$
      * $D_{KL}(P||Q)=0, \text{if P and Q are equal almost surely}$
      * KL divergence is asymmetric, i.e., $D_{KL}(P||Q)\ne D_{KL}(Q||P)$
    * Application: i.e. æ‰¾åˆ°ä¸€distribution Qï¼Œä½¿å¾—å…¶ä¸true distribution Pæœ€ç›¸ç¬¦(ä¸”P,Qç›¸äº’ç‹¬ç«‹): $\arg\min_{Q}D_{KL}(P||Q)$
    * ä¸Šè¿°ç›®çš„ç­‰ä»·äº$\arg\min_{Q}H(P, Q)=\arg\min_{Q}-E_{x\sim P}\left[\log Q(x)\right]$
  * Cross-Entropy can act as a Cost Function: log Loss/Cross-Entropy Loss
    * $H(\text{p, q})=-\sum_{i}p_{i}\log(q_{i})$
    * é‡‡ç”¨è‡ªç„¶åº•æ•°
    * æ¢åº•å…¬å¼: $log_2(x)=log(x)/log(2)$

|Index|Weather|Possibility|Code|Information|Message|
|---|---|---|---|---|---|
|0  |ğŸŒ |0.35|000|1.51 bits|3 bits|
|1  |â›… |0.35|001|1.51 bits|3 bits|
|2  |â˜ |0.1|010|3.32 bits|3 bits|
|3  |ğŸŒ¨ |0.1|011|3.32 bits|3 bits|
|4  |ğŸŒ§ |0.04|100|4.64 bits|3 bits|
|5  |ğŸŒ© |0.04|101|4.64 bits|3 bits|
|6  |ğŸŒª |0.01|110|6.64 bits|3 bits|
|7  |ğŸŒ« |0.01|111|6.64 bits|3 bits|

è¡¨2

|Index|Weather|Possibility|Code|Information|Message|
|---|---|---|---|---|---|
|0  |ğŸŒ |0.35|00|1.51 bits|2 bits|
|1  |â›… |0.35|01|1.51 bits|2 bits|
|2  |â˜ |0.1|100|3.32 bits|3 bits|
|3  |ğŸŒ¨ |0.1|101|3.32 bits|3 bits|
|4  |ğŸŒ§ |0.04|1100|4.64 bits|4 bits|
|5  |ğŸŒ© |0.04|1101|4.64 bits|4 bits|
|6  |ğŸŒª |0.01|11100|6.64 bits|5 bits|
|7  |ğŸŒ« |0.01|11101|6.64 bits|5 bits|

è¡¨3 (unambiguous code)

|Code|Bit|Note|
|---|---|---|
|00 |2  |  |
|01 |2  |  |
|100    |3  |å‰2ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰2ä½|
|101    |3  |  |
|1100   |4  |å‰3ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰3ä½|
|1101   |4  |  |
|11100  |5  |å‰4ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰4ä½|
|11101  |5  |  |

è¡¨4

|   |P(red)|P(Blue)|P(winning)|-log2(P(winning))|Entropy|
|---|---|---|---|---|---|
|ğŸ”´ğŸ”´ğŸ”´ğŸ”´|1  |0|1\*1\*1\*1=1|0+0+0+0=1|0   |
|ğŸ”´ğŸ”´ğŸ”´ğŸ”µ|0.75 |0.25|0.75\*0.75\*0.75\*0.75=0.105|0.415+0.415+0.415+2|0.81  
|ğŸ”´ğŸ”´ğŸ”µğŸ”µ|0.5  |0.5|0.5\*0.5\*0.5\*0.5=0.0625|1+1+1+1|1   |

è¡¨5

![å›¾4](https://image.slidesharecdn.com/featureselection-120719061056-phpapp02/95/feature-selection-14-638.jpg?cb=1389173732)

![å›¾5](https://cn.bing.com/th?id=OIP.g67516lIRLc0B93I7z5p-gHaFY&pid=Api&rs=1)

* More details about KL divergence
  * Given P, we want to find Q* that minizes the KL divergence
  * ç”±äºKL divergenceæ˜¯ä¸å¯¹ç§°çš„ï¼Œæœ‰ä¸¤ç§æœ€å°åŒ–
  * å‚è§å›¾6

![å›¾6](./figs/Minimizer-of-KL-Divergence.png)

#### Reference

1. [ğŸ“º Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)
2. [ğŸ“º Shannon Entropy and Information Gain](https://www.bilibili.com/video/av73032356?p=1)
3. [ğŸ“º Khan Academy | Shannon Entropy](https://www.bilibili.com/video/av73032356?p=2)
4. [ğŸ“º KL Divergence](https://www.bilibili.com/video/av73032356?p=4)
