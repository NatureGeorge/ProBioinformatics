# Mathematical ModelingğŸŒ€ğŸ”¥âš¡

```txt
Created Date: Thursday, September 5th 2019, 10:57:07 am
Author: ZeFeng Zhu
```

## Progress

> ğŸ‘¶ ğŸ‘¦ ğŸ§’ ğŸ‘¨ ğŸ‘´

|   |Mathematical Modeling|Pattern Recognition|Machine Learning|
|---|---|---|---|
|Basic Knowledge|ğŸ‘¦|ğŸ‘¦|ğŸ‘¶|
|Introduction|ğŸ‘¦|ğŸ‘¦|ğŸ‘¶|
|Practice|ğŸ‘¶|ğŸ‘¶|ğŸ‘¶|
|Pro|ğŸ‘¶|ğŸ‘¶|ğŸ‘¶|

## Some Note

> Noted that following note may be error-prone

### Entropy (Measuring Information)

#### Definition of Shannon's Entropy

* ä¿¡æ¯ä¸­æœ‰
  * æœ‰æ„ä¹‰ä¿¡æ¯(éå†—ä½™)
  * æ— æ„ä¹‰ä¿¡æ¯
  * å†—ä½™ä¿¡æ¯
* Bitä½œä¸ºä¿¡æ¯é‡(Information)çš„è¡¡é‡æ–¹å¼
  * Bit = 0 or 1 (æ˜¯æˆ–ä¸æ˜¯, å¤©ç„¶ç­‰æ¦‚ç‡)
  * Bit = Uncertainty divided by 2
* å¯¹äºBitè®¡ç®—æ–¹æ³•çš„ä¸¤ç§ç›´è§‚ç†è§£æ–¹å¼
  * Nç§æƒ…å†µ(state) -> logè®¡ç®— -> å¾—åˆ°ä¿¡æ¯é‡ $\log_{2}^{N}$
    * å¯ä»¥è¿™æ ·ç†è§£ï¼ŒNç§æƒ…å†µç­‰å¯èƒ½(Uncertainty divided by N, N is uncertainty reduction factor)ï¼ŒæŠŠæ‰€æœ‰æƒ…å†µèµ‹äºˆäºŒè¿›åˆ¶ç¼–ç , ä¸ºäº†å®Œæ•´æè¿°æ¯ä¸€æƒ…å†µï¼Œéœ€è¦çš„äºŒè¿›åˆ¶ä½æ•°å³æ˜¯ä¼ é€’è¯¥æƒ…å†µæ‰€éœ€çš„ä¿¡æ¯é‡
    * åœ¨æ­¤ç§ç†è§£æ–¹å¼ä¸‹
      * Nåªèƒ½ä¸ºæ­£æ•´æ•°?
      * åªé€‚ç”¨äºå„ä¸ªæƒ…å†µ(state)<å¯èƒ½æ€§/æ¦‚ç‡>å‡ç­‰çš„æƒ…å†µ?
  * ç›´æ¥logåŒ–æ¦‚ç‡ (MORE GENERAL) $-log_2^{p}$
* ä»logè®¡ç®—æ¥çœ‹, å¯¹äºæ¦‚ç‡è¶Šå¤§çš„äº‹ä»¶, å…¶å¯¹åº”ä¿¡æ¯bitè¶Šä½; stateè¶Šå°‘å¯¹åº”ä¿¡æ¯bitä¹Ÿè¶Šä½ã€‚åä¹‹è¶Šå¤§ã€‚æ„å‘³ç€è¶Šç¡®å®šçš„äº‹æƒ…bitè¶Šä½(ä¿¡æ¯é‡è¶Šå°‘)ï¼Œåä¹‹è¶Šå¤§ã€‚
* Measure the average amount of information
  * ç›¸å½“äºæ•°å­¦æœŸæœ›
  * $-\sum_{i}p_{i}log_{2}(p_{i})$
  * That's it, the formula calculates the entropy, which measures the uncertainty.
  * $H(\text{p}) = -\sum_{i}p_{i}log_{2}(p_{i})$, $\text{p}$ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    * it tells how unpredictable the probability distribution is.
    * ä¸”åœ¨è¿™é‡Œçš„ä¿¡æ¯éƒ½æ˜¯æœ‰æ„ä¹‰ä¿¡æ¯ï¼Œæ— æ„ä¹‰æˆ–å†—ä½™ä¿¡æ¯å¯ä»¥çœ‹ä½œæ˜¯ä¸ºä¼ é€’æœ‰æ„ä¹‰ä¿¡æ¯è€Œäº§ç”Ÿçš„æˆæœ¬ï¼Œä¼ é€’çš„ä¿¡æ¯çš„é›†åˆç§°ä¸ºæ¶ˆæ¯(message)

|Index|Weather|Possibility|Code|Information|Message|
|---|---|---|---|---|---|
|0  |ğŸŒ |0.125|000|3 bits|3 bits|
|1  |â›… |0.125|001|3 bits|3 bits|
|2  |â˜ |0.125|010|3 bits|3 bits|
|3  |ğŸŒ¨ |0.125|011|3 bits|3 bits|
|4  |ğŸŒ§ |0.125|100|3 bits|3 bits|
|5  |ğŸŒ© |0.125|101|3 bits|3 bits|
|6  |ğŸŒª |0.125|110|3 bits|3 bits|
|7  |ğŸŒ« |0.125|111|3 bits|3 bits|

è¡¨1

* Cross-Entropy
  * å¯ä»¥ç†è§£ä¸ºä¼ é€’æ¶ˆæ¯(message)æ‰€éœ€çš„æœŸæœ›ä¿¡æ¯é‡?
  * è¡¨2ä¸­ $\text{Entropy} = 2.23 \,\text{bits},\, \text{Cross-Entropy} = 3\,\text{bits}$
  * è¡¨3ä¸­ $\text{Cross-Entropy} = 2.42\,\text{bits}$
  * $H(\text{p, q})=-\sum_{i}p_{i}\log_{2}(q_{i})$, $\text{p}$ä¸ºtrueæ¦‚ç‡åˆ†å¸ƒ, $\text{q}$ä¸ºpredictedæ¦‚ç‡åˆ†å¸ƒ
  * æåŠçš„äºŒè¡¨çš„Messageåˆ—å³ä¸ºpredicted distributionï¼Œç”±Codeåˆ—å†³å®š
  * predicted possibilityåŠ å’Œä¸ä¸€å®šä¸º1
  * å¦‚æœé¢„æµ‹è‰¯å¥½(ä¸¤distributionç›¸ç­‰)ï¼Œåˆ™Cross-Entropyä¸Entropyè®¡ç®—æ•°å€¼ç›¸ç­‰; å¦‚æœpredicted distributionä¸true distributionå­˜åœ¨å·®å¼‚ï¼Œåˆ™Cross-Entropyå°†ä¼šæ¯”Entropyå¤§(åœ¨messageè¿›è¡Œåˆç†ç¼–ç çš„æƒ…å†µä¸‹?)
    * $\text{Cross-Entropy}-\text{Entropy} \Rightarrow \text{Relative Entropy} \Rightarrow \text{Kullback-Leibler divergence}$
    * $\text{Cross-Entropy} = \text{Entropy} + \text{K-L divergence}$
  * K-L divergence:
    * $D_{KL}(\text{p}||\text{q})=H(\text{p, q})-H(\text{p})=-\sum_{i}p_{i}\log_{2}(q_{i})+\sum_{i}p_{i}\log_{2}(p_{i})=\sum_{i}p_{i}[log_{2}(p_{i})-log_2(q_{i})]=\sum_{i}p_{i}\log_{2}^{\cfrac{p_{i}}{q_{i}}}$
    * ç‰¹æ®Šæƒ…å½¢: $D(p||q)=p\log^{\cfrac{p}{q}}+(1-p)\log^{\cfrac{1-p}{1-q}}$
  * Cross-Entropy can act as a Cost Function: log Loss/Cross-Entropy Loss
    * $H(\text{p, q})=-\sum_{i}p_{i}\log(q_{i})$
    * é‡‡ç”¨è‡ªç„¶åº•æ•°
    * æ¢åº•å…¬å¼: $log_2(x)=log(x)/log(2)$

|Index|Weather|Possibility|Code|Information|Message|
|---|---|---|---|---|---|
|0  |ğŸŒ |0.35|000|1.51 bits|3 bits|
|1  |â›… |0.35|001|1.51 bits|3 bits|
|2  |â˜ |0.1|010|3.32 bits|3 bits|
|3  |ğŸŒ¨ |0.1|011|3.32 bits|3 bits|
|4  |ğŸŒ§ |0.04|100|4.64 bits|3 bits|
|5  |ğŸŒ© |0.04|101|4.64 bits|3 bits|
|6  |ğŸŒª |0.01|110|6.64 bits|3 bits|
|7  |ğŸŒ« |0.01|111|6.64 bits|3 bits|

è¡¨2

|Index|Weather|Possibility|Code|Information|Message|
|---|---|---|---|---|---|
|0  |ğŸŒ |0.35|00|1.51 bits|2 bits|
|1  |â›… |0.35|01|1.51 bits|2 bits|
|2  |â˜ |0.1|100|3.32 bits|3 bits|
|3  |ğŸŒ¨ |0.1|101|3.32 bits|3 bits|
|4  |ğŸŒ§ |0.04|1100|4.64 bits|4 bits|
|5  |ğŸŒ© |0.04|1101|4.64 bits|4 bits|
|6  |ğŸŒª |0.01|11100|6.64 bits|5 bits|
|7  |ğŸŒ« |0.01|11101|6.64 bits|5 bits|

è¡¨3 (unambiguous code)

|Code|Bit|Note|
|---|---|---|
|00 |2  |  |
|01 |2  |  |
|100    |3  |å‰2ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰2ä½|
|101    |3  |  |
|1100   |4  |å‰3ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰3ä½|
|1101   |4  |  |
|11100  |5  |å‰4ä½å·²è¢«ä¸Šä¸€çº§å ï¼Œé‡‡ç”¨è¿›ä½ç”Ÿæˆæ–°å‰4ä½|
|11101  |5  |  |

è¡¨4

#### Reference

1. [ğŸ“º Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)
